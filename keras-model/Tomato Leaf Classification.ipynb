{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import image\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "print(tensorflow.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Tomato___Bacterial_spot\n",
      "1 Tomato___Early_blight\n",
      "2 Tomato___healthy\n",
      "3 Tomato___Late_blight\n",
      "4 Tomato___Leaf_Mold\n",
      "5 Tomato___Septoria_leaf_spot\n",
      "6 Tomato___Spider_mites Two-spotted_spider_mite\n",
      "7 Tomato___Target_Spot\n",
      "8 Tomato___Tomato_Yellow_Leaf_Curl_Virus\n",
      "(8550, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "#define parameters for the Convolutional Neural Network\n",
    "image_size = 128\n",
    "depth = 3 #3 for RGB, 1 for Grayscale\n",
    "num_categories = 9\n",
    "num_epochs = 30\n",
    "batch_size = 32\n",
    "learning_rate = 0.0005\n",
    "\n",
    "#load images from PlantVillage folder\n",
    "data_directory = \"C:/Users/Reuben/Desktop/Computer Vision/Datasets/Tomato Leaf Raw\"\n",
    "categories = []\n",
    "folder_list = os.scandir(data_directory)\n",
    "for folder in folder_list:\n",
    "    categories.append(folder.name)\n",
    "    \n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for category in categories:\n",
    "    path = os.path.join(data_directory, category)\n",
    "    \n",
    "    class_num = categories.index(category)\n",
    "    print(class_num, category)\n",
    "        \n",
    "    for img in os.listdir(path)[:950]:\n",
    "        img = tensorflow.keras.preprocessing.image.load_img(os.path.join(path, img), target_size=(128, 128))\n",
    "        new_img = tensorflow.keras.preprocessing.image.img_to_array(img)\n",
    "        #input images into NumPy arrays Data and Label\n",
    "        data.append(new_img)\n",
    "        labels.append(class_num)\n",
    "        \n",
    "data = np.array(data, dtype=np.float64)/255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "labels = np.eye(num_categories)[labels] \n",
    "\n",
    "data, labels = shuffle(data, labels, random_state=0)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[0:5985]\n",
    "train_labels = labels[0:5985]\n",
    "\n",
    "test_data = data[5985:8550]\n",
    "test_labels = labels[5985:8550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=25, width_shift_range=0.1,\n",
    "    height_shift_range=0.1, shear_range=0.2, \n",
    "    zoom_range=0.2,horizontal_flip=True, \n",
    "    fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Reuben\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "inputShape = (image_size, image_size, depth)\n",
    "chanDim = -1\n",
    "if K.image_data_format() == \"channels_first\":\n",
    "    inputShape = (depth, height, width)\n",
    "    chanDim = 1\n",
    "    \n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", activation='relu', input_shape=inputShape))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", activation='relu'))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", activation='relu'))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_categories))\n",
    "model.add(Activation(\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 128, 128, 32)      896       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128, 128, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 42, 42, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 42, 42, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 42, 42, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 42, 42, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 42, 42, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 42, 42, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 21, 21, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 21, 21, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 21, 21, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 21, 21, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 21, 21, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 21, 21, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              13108224  \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 9)                 9225      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 13,400,969\n",
      "Trainable params: 13,398,089\n",
      "Non-trainable params: 2,880\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr=learning_rate, decay=learning_rate / num_epochs)\n",
    "# distribution\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:From C:\\Users\\Reuben\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.2448 - acc: 0.9119 - ETA: 8s - loss: 0.2544 - ETA: 5s - loss: 0.2513 - acc:  - ETA: 4s - loss: 0.2Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 5s 2ms/sample - loss: 1.3338 - acc: 0.8017\n",
      "187/187 [==============================] - 47s 249ms/step - loss: 0.2441 - acc: 0.9121 - val_loss: 1.4035 - val_acc: 0.8017\n",
      "Epoch 2/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9392- ETA: 4s - loss:Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 4s 2ms/sample - loss: 1.0582 - acc: 0.8220\n",
      "187/187 [==============================] - 40s 215ms/step - loss: 0.1629 - acc: 0.9393 - val_loss: 1.0898 - val_acc: 0.8220\n",
      "Epoch 3/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9475Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 4s 2ms/sample - loss: 0.4599 - acc: 0.8581\n",
      "187/187 [==============================] - 40s 215ms/step - loss: 0.1445 - acc: 0.9475 - val_loss: 0.5707 - val_acc: 0.8581\n",
      "Epoch 4/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9565Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 4s 2ms/sample - loss: 0.5063 - acc: 0.8880\n",
      "187/187 [==============================] - 40s 215ms/step - loss: 0.1164 - acc: 0.9566 - val_loss: 0.5482 - val_acc: 0.8880\n",
      "Epoch 5/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9630Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 4s 2ms/sample - loss: 1.1682 - acc: 0.8284\n",
      "187/187 [==============================] - 40s 215ms/step - loss: 0.1039 - acc: 0.9629 - val_loss: 1.3013 - val_acc: 0.8284\n",
      "Epoch 6/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9596  ETA: 16s - loss: 0.1231 - acc:   - ETA:  - ETA: 0s - loss: 0.1091 - acc: 0.959 - ETA: 0s - loss: 0.1095 - acc: 0.9595Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 4s 2ms/sample - loss: 0.0881 - acc: 0.9503\n",
      "187/187 [==============================] - 40s 214ms/step - loss: 0.1093 - acc: 0.9596 - val_loss: 0.1691 - val_acc: 0.9503\n",
      "Epoch 7/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9668Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 4s 2ms/sample - loss: 0.1731 - acc: 0.9292\n",
      "187/187 [==============================] - 40s 215ms/step - loss: 0.0882 - acc: 0.9669 - val_loss: 0.2588 - val_acc: 0.9292\n",
      "Epoch 8/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9712Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 10s 4ms/sample - loss: 0.3809 - acc: 0.9043\n",
      "187/187 [==============================] - 65s 348ms/step - loss: 0.0790 - acc: 0.9711 - val_loss: 0.5048 - val_acc: 0.9043\n",
      "Epoch 9/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9756Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 10s 4ms/sample - loss: 0.4460 - acc: 0.8897\n",
      "187/187 [==============================] - 87s 463ms/step - loss: 0.0685 - acc: 0.9756 - val_loss: 0.6177 - val_acc: 0.8897\n",
      "Epoch 10/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9724Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 10s 4ms/sample - loss: 0.1560 - acc: 0.9339\n",
      "187/187 [==============================] - 87s 466ms/step - loss: 0.0734 - acc: 0.9723 - val_loss: 0.2794 - val_acc: 0.9339\n",
      "Epoch 11/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9764Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 9s 4ms/sample - loss: 0.2384 - acc: 0.9209\n",
      "187/187 [==============================] - 86s 460ms/step - loss: 0.0660 - acc: 0.9764 - val_loss: 0.3422 - val_acc: 0.9209\n",
      "Epoch 12/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9789Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 10s 4ms/sample - loss: 0.0869 - acc: 0.9602\n",
      "187/187 [==============================] - 86s 462ms/step - loss: 0.0582 - acc: 0.9789 - val_loss: 0.1237 - val_acc: 0.9602\n",
      "Epoch 13/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9792Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 10s 4ms/sample - loss: 0.4415 - acc: 0.9111\n",
      "187/187 [==============================] - 87s 463ms/step - loss: 0.0571 - acc: 0.9793 - val_loss: 0.4807 - val_acc: 0.9111\n",
      "Epoch 14/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9791Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 10s 4ms/sample - loss: 0.2846 - acc: 0.9321\n",
      "187/187 [==============================] - 87s 463ms/step - loss: 0.0571 - acc: 0.9791 - val_loss: 0.3746 - val_acc: 0.9321\n",
      "Epoch 15/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9824Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 10s 4ms/sample - loss: 0.1414 - acc: 0.9352\n",
      "187/187 [==============================] - 87s 463ms/step - loss: 0.0481 - acc: 0.9824 - val_loss: 0.2300 - val_acc: 0.9352\n",
      "Epoch 16/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9822Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 744us/sample - loss: 0.0792 - acc: 0.9543\n",
      "187/187 [==============================] - 30s 161ms/step - loss: 0.0512 - acc: 0.9820 - val_loss: 0.1479 - val_acc: 0.9543\n",
      "Epoch 17/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9813Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 770us/sample - loss: 0.1195 - acc: 0.9375\n",
      "187/187 [==============================] - 22s 117ms/step - loss: 0.0503 - acc: 0.9814 - val_loss: 0.2133 - val_acc: 0.9375\n",
      "Epoch 18/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9859Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 825us/sample - loss: 0.1109 - acc: 0.9553\n",
      "187/187 [==============================] - 24s 127ms/step - loss: 0.0411 - acc: 0.9860 - val_loss: 0.2090 - val_acc: 0.9553\n",
      "Epoch 19/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9845Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 847us/sample - loss: 0.4720 - acc: 0.9012\n",
      "187/187 [==============================] - 28s 150ms/step - loss: 0.0441 - acc: 0.9845 - val_loss: 0.6090 - val_acc: 0.9012\n",
      "Epoch 20/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9857- ETA: 2s - loss: 0.04Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 872us/sample - loss: 0.2196 - acc: 0.9254\n",
      "187/187 [==============================] - 29s 158ms/step - loss: 0.0406 - acc: 0.9857 - val_loss: 0.3609 - val_acc: 0.9254\n",
      "Epoch 21/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9852Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 877us/sample - loss: 0.1857 - acc: 0.9360\n",
      "187/187 [==============================] - 29s 155ms/step - loss: 0.0389 - acc: 0.9852 - val_loss: 0.3006 - val_acc: 0.9360\n",
      "Epoch 22/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9868Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 905us/sample - loss: 0.4726 - acc: 0.8820\n",
      "187/187 [==============================] - 29s 157ms/step - loss: 0.0365 - acc: 0.9868 - val_loss: 0.8820 - val_acc: 0.8820\n",
      "Epoch 23/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9857Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 847us/sample - loss: 0.2771 - acc: 0.9192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 27s 144ms/step - loss: 0.0398 - acc: 0.9857 - val_loss: 0.5347 - val_acc: 0.9192\n",
      "Epoch 24/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9860Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 829us/sample - loss: 0.4362 - acc: 0.9271\n",
      "187/187 [==============================] - 25s 132ms/step - loss: 0.0382 - acc: 0.9861 - val_loss: 0.4761 - val_acc: 0.9271\n",
      "Epoch 25/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9883Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 818us/sample - loss: 0.0526 - acc: 0.9701\n",
      "187/187 [==============================] - 23s 125ms/step - loss: 0.0354 - acc: 0.9882 - val_loss: 0.1018 - val_acc: 0.9701\n",
      "Epoch 26/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9852Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 817us/sample - loss: 0.5583 - acc: 0.9202\n",
      "187/187 [==============================] - 23s 124ms/step - loss: 0.0407 - acc: 0.9852 - val_loss: 0.5180 - val_acc: 0.9202\n",
      "Epoch 27/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9879Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 876us/sample - loss: 0.5694 - acc: 0.9241\n",
      "187/187 [==============================] - 27s 145ms/step - loss: 0.0347 - acc: 0.9878 - val_loss: 0.5011 - val_acc: 0.9241\n",
      "Epoch 28/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9886Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 1ms/sample - loss: 0.0885 - acc: 0.9653\n",
      "187/187 [==============================] - 38s 204ms/step - loss: 0.0319 - acc: 0.9886 - val_loss: 0.1673 - val_acc: 0.9653\n",
      "Epoch 29/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9884Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 1ms/sample - loss: 0.0598 - acc: 0.9668\n",
      "187/187 [==============================] - 36s 193ms/step - loss: 0.0328 - acc: 0.9884 - val_loss: 0.0991 - val_acc: 0.9668\n",
      "Epoch 30/30\n",
      "186/187 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9885Epoch 1/30\n",
      "2565/187 [===========================================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 1ms/sample - loss: 0.0612 - acc: 0.9716\n",
      "187/187 [==============================] - 37s 197ms/step - loss: 0.0310 - acc: 0.9885 - val_loss: 0.0881 - val_acc: 0.9716\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    datagen.flow(train_data, train_labels, batch_size=batch_size),\n",
    "    validation_data=(test_data, test_labels),\n",
    "    steps_per_epoch=len(train_data) // batch_size,\n",
    "    epochs=num_epochs, verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('C:/Users/Reuben/Desktop/Computer Vision/TF Scale/Leaf Detection/Models/Tomato Leaf Classification.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs\n",
    "tfjs.converters.save_keras_model(model, \"C:/Users/Reuben/Desktop/Computer Vision/TF Scale/Leaf Detection/Models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
